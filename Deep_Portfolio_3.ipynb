{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f46b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925cbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"transforming-fashion-2025/Dataset/Train/\"\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"images\")\n",
    "LABEL_DIR = os.path.join(BASE_DIR, \"labels\")\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: \"sunglass\", 1: \"hat\", 2: \"jacket\", 3: \"shirt\", 4: \"pants\",\n",
    "    5: \"shorts\", 6: \"skirt\", 7: \"dress\", 8: \"bag\", 9: \"shoe\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95403cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(label_path):\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    annotations = []\n",
    "    for line in lines:\n",
    "        cls, cx, cy, w, h = map(float, line.strip().split())\n",
    "        annotations.append({\n",
    "            'class_id': int(cls),\n",
    "            'center_x': cx,\n",
    "            'center_y': cy,\n",
    "            'width': w,\n",
    "            'height': h\n",
    "        })\n",
    "    return annotations\n",
    "\n",
    "def plot_image_with_boxes(image_path, annotations):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w, _ = img.shape\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for ann in annotations:\n",
    "        cx = ann['center_x'] * w\n",
    "        cy = ann['center_y'] * h\n",
    "        bw = ann['width'] * w\n",
    "        bh = ann['height'] * h\n",
    "        x0 = cx - bw / 2\n",
    "        y0 = cy - bh / 2\n",
    "        rect = plt.Rectangle((x0, y0), bw, bh, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        class_name = CLASS_NAMES.get(ann['class_id'], str(ann['class_id']))\n",
    "        ax.text(x0, y0 - 5, class_name, color='white', fontsize=10,\n",
    "                bbox=dict(facecolor='black', alpha=0.6, edgecolor='none'))\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d782ff",
   "metadata": {},
   "source": [
    "## Alle afbeeldingen en labels samen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ff0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.endswith((\".jpg\", \".png\", \".jpeg\"))])\n",
    "\n",
    "\n",
    "records = []\n",
    "\n",
    "print(\"Labels laden...\")\n",
    "for img_file in tqdm(image_files):\n",
    "    img_path = os.path.join(IMAGE_DIR, img_file)\n",
    "    label_path = os.path.join(LABEL_DIR, os.path.splitext(img_file)[0] + \".txt\")\n",
    "    if not os.path.exists(label_path):\n",
    "        continue\n",
    "    annotations = load_annotations(label_path)\n",
    "    for ann in annotations:\n",
    "        ann[\"image\"] = img_file\n",
    "        records.append(ann)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927d444",
   "metadata": {},
   "source": [
    "## Klasseverdeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.countplot(data=df, x=\"class_id\", palette=\"Set2\")\n",
    "plt.title(\"Aantal objecten per klasse\")\n",
    "plt.xticks(ticks=range(10), labels=[CLASS_NAMES[i] for i in range(10)], rotation=45)\n",
    "plt.xlabel(\"Klasse\")\n",
    "plt.ylabel(\"Aantal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8666102",
   "metadata": {},
   "source": [
    "## Voorbeeld afbeeldingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VIS = 5  # Aantal afbeeldingen om te visualiseren\n",
    "\n",
    "print(f\"{N_VIS} voorbeeldafbeeldingen met bounding-boxes:\")\n",
    "for fname in random.sample(image_files, k=min(N_VIS, len(image_files))):\n",
    "    label_path = os.path.join(LABEL_DIR, os.path.splitext(fname)[0] + \".txt\")\n",
    "    anns = load_annotations(label_path)\n",
    "    plot_image_with_boxes(os.path.join(IMAGE_DIR, fname), anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5778f74",
   "metadata": {},
   "source": [
    "## Co-occurrence matrix (welke items komen samen in één afbeelding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary matrix: rows = images, columns = classes\n",
    "pivot_df = df.pivot_table(index=\"image\", columns=\"class_id\", aggfunc=\"size\", fill_value=0)\n",
    "co_matrix = (pivot_df.T @ pivot_df)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(co_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n",
    "            xticklabels=[CLASS_NAMES[i] for i in range(10)],\n",
    "            yticklabels=[CLASS_NAMES[i] for i in range(10)])\n",
    "plt.title(\"Co-occurrence matrix van objectklassen\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9bf539",
   "metadata": {},
   "source": [
    "## Aantal objecten per afbeelding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_counts = df.groupby(\"image\").size()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(object_counts, bins=20)\n",
    "plt.title(\"Aantal objecten per afbeelding\")\n",
    "plt.xlabel(\"Aantal objecten\")\n",
    "plt.ylabel(\"Aantal afbeeldingen\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149eed5",
   "metadata": {},
   "source": [
    "## Overeenkomende items in captioning-dataset en de objectdetectiedatase\n",
    "\n",
    "- De objectdetectie-dataset bevat 10 klassen:\n",
    "0 = sunglass\n",
    "1 = hat\n",
    "2 = jacket\n",
    "3 = shirt\n",
    "4 = pants\n",
    "5 = shorts\n",
    "6 = skirt\n",
    "7 = dress\n",
    "8 = bag\n",
    "9 = shoe\n",
    "\n",
    "- De captioning dataset bevat Engels tekstuele bijschriften van kledingstukken. Veelvoorkomende begrippen in die bijschriften zijn:\n",
    "“dress”\n",
    "“jacket”\n",
    "“skirt”\n",
    "“shirt”\n",
    "“pants”\n",
    "“shorts”\n",
    "“shoes”\n",
    "“bag”\n",
    "“hat”\n",
    "“sunglasses”\n",
    "\n",
    "Conclusie: alle 10 klassen van de objectdetectie-dataset komen ook voor als modetermen in de captioning-dataset. Ze zijn dus volledig overlappend en compatibel voor multimodale taken zoals bijschriften genereren per gedetecteerd object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125bb6d",
   "metadata": {},
   "source": [
    "## Belangrijkste bevindingen van de EDA\n",
    "\n",
    "- **Klasseverdeling:** Alle 10 objectklassen (zoals 'sunglass', 'hat', 'jacket', etc.) zijn goed vertegenwoordigd in de dataset, maar sommige klassen komen vaker voor dan andere. Vooral 'shirt', 'shoe' en 'jacket' zijn veelvoorkomend.\n",
    "\n",
    "- **Aantal objecten per afbeelding:** De meeste afbeeldingen bevatten tussen de 2 en 5 objecten, met enkele uitschieters naar boven.\n",
    "\n",
    "- **Co-occurrence matrix:** Bepaalde kledingstukken komen vaak samen voor in één afbeelding, bijvoorbeeld 'shirt' met 'pants' of 'shoe' met 'dress'. Dit wijst op realistische combinaties van kledingstukken.\n",
    "\n",
    "- **Voorbeeldafbeeldingen:** De bounding boxes zijn over het algemeen goed geplaatst en de annotaties zijn visueel controleerbaar.\n",
    "\n",
    "- **Overlap met captioning-dataset:** Alle objectklassen uit de objectdetectie-dataset komen ook als termen voor in de captioning-dataset. Dit maakt de datasets volledig compatibel voor multimodale taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba4e93",
   "metadata": {},
   "source": [
    "# **Deel 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334c308",
   "metadata": {},
   "source": [
    "# Objectdetectie YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split images into train and validation sets\n",
    "train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create directories for train and validation sets\n",
    "train_img_dir = os.path.join(BASE_DIR, \"train\", \"images\")\n",
    "train_label_dir = os.path.join(BASE_DIR, \"train\", \"labels\")\n",
    "val_img_dir = os.path.join(BASE_DIR, \"val\", \"images\")\n",
    "val_label_dir = os.path.join(BASE_DIR, \"val\", \"labels\")\n",
    "\n",
    "os.makedirs(train_img_dir, exist_ok=True)\n",
    "os.makedirs(train_label_dir, exist_ok=True)\n",
    "os.makedirs(val_img_dir, exist_ok=True)\n",
    "os.makedirs(val_label_dir, exist_ok=True)\n",
    "\n",
    "# Copy images and labels to the new directories\n",
    "for img_file in tqdm(train_files, desc=\"Copying training files\"):\n",
    "    label_file = os.path.splitext(img_file)[0] + \".txt\"\n",
    "    os.rename(os.path.join(IMAGE_DIR, img_file), os.path.join(train_img_dir, img_file))\n",
    "    if os.path.exists(os.path.join(LABEL_DIR, label_file)):\n",
    "      os.rename(os.path.join(LABEL_DIR, label_file), os.path.join(train_label_dir, label_file))\n",
    "\n",
    "\n",
    "for img_file in tqdm(val_files, desc=\"Copying validation files\"):\n",
    "    label_file = os.path.splitext(img_file)[0] + \".txt\"\n",
    "    os.rename(os.path.join(IMAGE_DIR, img_file), os.path.join(val_img_dir, img_file))\n",
    "    if os.path.exists(os.path.join(LABEL_DIR, label_file)):\n",
    "      os.rename(os.path.join(LABEL_DIR, label_file), os.path.join(val_label_dir, label_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update YOLO data dictionary with new directory paths\n",
    "yolo_data_yaml = {\n",
    "    'train': train_img_dir,\n",
    "    'val': val_img_dir,\n",
    "    'nc': 10,\n",
    "    'names': [CLASS_NAMES[i] for i in range(10)]\n",
    "}\n",
    "\n",
    "# Save the updated YAML file\n",
    "yaml_path = \"custom_yolo_data.yaml\"\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(yolo_data_yaml, f)\n",
    "\n",
    "print(f\"Updated {yaml_path} with train: {train_img_dir} and val: {val_img_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a234f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=10,\n",
    "    imgsz=640,\n",
    "    project='yolo_train',\n",
    "    name='custom_clothes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c136068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad het getrainde model\n",
    "model = YOLO('/content/yolo_train/custom_clothes4/weights/best.pt')\n",
    "\n",
    "# Evalueer het model op de validatieset\n",
    "# Gebruik de 'metrics' methode na een 'val' run\n",
    "results = model.val(data=yaml_path)\n",
    "\n",
    "print(\"\\nEvaluatie Resultaten:\")\n",
    "print(f\"  mAP@0.5: {results.box.map50:.4f}\")\n",
    "print(f\"  mAP@0.5:0.95: {results.box.map:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9a049",
   "metadata": {},
   "source": [
    "### Hoe doet het model voorspellingen?\n",
    "Het model dat gebruikt wordt is een YOLOv8 objectdetectiemodel. Na het trainen wordt het model geladen met het pad naar het beste gewichtenbestand (`YOLO('/content/yolo_train/custom_clothes4/weights/best.pt')`). Om voorspellingen te doen, geef je een nieuwe afbeelding aan het model. Het model analyseert de afbeelding en geeft voor elk gedetecteerd object een bounding box, een klassenlabel (zoals 'shirt', 'shoe', etc.) en een waarschijnlijkheidsscore terug. Dit gebeurt automatisch voor alle objecten die het model herkent in de afbeelding.\n",
    "\n",
    "\n",
    "### Hoe worden de afbeeldingen voorbewerkt?\n",
    "Voor zowel training als voorspellen worden de afbeeldingen automatisch geschaald naar een vaste resolutie van 640x640 pixels (`imgsz=640`). De pixelwaarden worden genormaliseerd zodat het model sneller en stabieler kan leren. Tijdens training worden er ook augmentaties toegepast, zoals draaien, spiegelen en croppen, om het model robuuster te maken tegen variaties in de data. Deze voorbewerkingen worden automatisch uitgevoerd door de YOLO pipeline.\n",
    "\n",
    "\n",
    "### Welke stappen onderneem je voor feature engineering?\n",
    "Bij YOLO is handmatige feature engineering nauwelijks nodig. Het model leert zelf welke kenmerken belangrijk zijn voor het herkennen van objecten. Wel worden de bounding boxes genormaliseerd (waarden tussen 0 en 1 ten opzichte van de afbeeldingsgrootte) en worden augmentaties toegepast om de dataset te verrijken. Dit helpt het model om beter te generaliseren naar nieuwe, ongeziene situaties.\n",
    "\n",
    "\n",
    "### Keuze van hyperparameters, optimizer en aantal epochs\n",
    "- **Epochs:** Er is gekozen voor 10 epochs (`epochs=10`). Dit is een praktisch startpunt om snel te zien of het model leert. Meer epochs kunnen de prestaties verbeteren, maar kosten meer rekentijd.\n",
    "- **Optimizer:** YOLOv8 gebruikt standaard de AdamW-optimizer. Deze optimizer is geschikt voor vision-taken omdat hij adaptief leert en gewichtsregularisatie toepast, wat overfitting helpt voorkomen.\n",
    "- **imgsz:** De afbeeldingsgrootte is 640 pixels. Dit is een goede balans tussen nauwkeurigheid en rekentijd; grotere afbeeldingen kunnen nauwkeuriger zijn, maar zijn ook zwaarder om te trainen.\n",
    "- **Andere parameters** zoals batch size zijn standaard gelaten, omdat deze al goed zijn afgestemd voor YOLO.\n",
    "\n",
    "\n",
    "### Beschrijving van de lossfunctie\n",
    "De lossfunctie van YOLO bestaat uit drie hoofdonderdelen:\n",
    "- **Bounding box regression loss:** Meet hoe goed de voorspelde bounding boxes overeenkomen met de echte (meestal met CIoU- of GIoU-loss).\n",
    "- **Objectness loss:** Meet of het model correct voorspelt of er een object aanwezig is in een bepaald gebied van de afbeelding.\n",
    "- **Classificatie loss:** Meet of het model de juiste klasse voorspelt voor elk gedetecteerd object.\n",
    "\n",
    "De totale loss is een gewogen som van deze drie onderdelen. Tijdens het trainen probeert het model deze totale loss te minimaliseren, zodat het steeds betere en nauwkeurigere voorspellingen doet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22b0c7",
   "metadata": {},
   "source": [
    "# **Deel 2&3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d2508",
   "metadata": {},
   "source": [
    "## Importeren van benodigde bibliotheken\n",
    "\n",
    "In deze stap worden alle benodigde bibliotheken ingeladen die essentieel zijn voor het bouwen, trainen en evalueren van een image captioning-model.\n",
    "\n",
    "We gebruiken:\n",
    "- huggingface_hub en getpass voor veilige toegang tot modellen en datasets via Hugging Face\n",
    "- datasets om kant-en-klare image-text datasets zoals H&M Captions in te laden\n",
    "- torchvision en PIL voor het transformeren en visualiseren van afbeeldingsdata\n",
    "- transformers om het ViT-GPT2 encoder-decoder model te gebruiken\n",
    "- torch en DataLoader om met batches te trainen en het model efficiënt te verwerken\n",
    "- evaluate voor de berekening van evaluatiemetrieken zoals BLEU-score\n",
    "- tqdm voor een overzichtelijke voortgangsbalk tijdens training\n",
    "- matplotlib en random voor het tonen en willekeurig selecteren van afbeeldingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f862a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from transformers.models.blip import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f5c95",
   "metadata": {},
   "source": [
    "## Inloggen bij Hugging Face\n",
    "\n",
    "Om toegang te krijgen tot bepaalde datasets en modellen op het Hugging Face platform is authenticatie vereist. We loggen daarom in met een persoonlijk access token. Het token wordt via getpass ingevoerd zodat het niet zichtbaar is in het notebook. Dit verhoogt de veiligheid, vooral wanneer het notebook gedeeld of opgeslagen wordt in de cloud of in een publieke repository. Zonder deze stap kan het laden van beschermde datasets of modellen mislukken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e246c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login zonder token zichtbaar te maken in je notebook\n",
    "token = getpass.getpass(\"Voer je Hugging Face access token in: \")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4eeea0",
   "metadata": {},
   "source": [
    "## Laden en splitsen van de H&M Fashion Captions dataset\n",
    "\n",
    "We laden de dataset 'tomytjandra/h-and-m-fashion-caption-12k' via Hugging Face. Deze dataset bevat meer dan 12.000 afbeeldingen van kledingstukken met bijbehorende Engelstalige productomschrijvingen. Elke afbeelding is gekoppeld aan één tekstuele beschrijving, wat ideaal is voor het trainen van een image captioning-model in een supervised learning setting.\n",
    "\n",
    "Na het laden splitsen we de dataset in een trainings- en validatieset. Dit doen we met een verhouding van 90% training en 10% validatie, zodat het model geleerd gedrag kan generaliseren naar nieuwe, ongeziene data.\n",
    "\n",
    "Omdat het trainen van grote datasets in Google Colab kan leiden tot geheugenproblemen, beperken we het aantal trainingsvoorbeelden tot 2000 en het aantal validatievoorbeelden tot 300. Deze reductie maakt het mogelijk om het model efficiënt te trainen en evalueren binnen de beperkingen van de beschikbare hardware, zonder daarbij de representativiteit van de data te verliezen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07585c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srlam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\srlam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%pip install tf-keras\n",
    "#!pip install transformers datasets evaluate torchvision huggingface_hub --quiet\n",
    "# Laad de H&M captioning dataset\n",
    "dataset = load_dataset(\"tomytjandra/h-and-m-fashion-caption-12k\")\n",
    "\n",
    "# Split in train/test\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = dataset[\"train\"].select(range(2000))\n",
    "val_dataset = dataset[\"test\"].select(range(300))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9bb5d",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab77367",
   "metadata": {},
   "source": [
    "## Voorbewerking van afbeeldingsdata\n",
    "\n",
    "In deze stap definiëren we een transformatiepipeline voor de afbeeldingen zodat ze geschikt zijn als input voor het Vision Transformer model (ViT). Omdat ViT-modellen alleen werken met afbeeldingen van vaste afmetingen, schalen we alle afbeeldingen naar 224 bij 224 pixels.\n",
    "\n",
    "Daarnaast converteren we elke afbeelding naar een PyTorch-tensor, wat nodig is voor verwerking in het neurale netwerk. Deze stappen zijn verplicht voordat de afbeeldingen als input kunnen worden gebruikt in het encoder-gedeelte van het model. Zonder uniforme grootte en tensorrepresentatie zou het model fouten geven tijdens training of inferentie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT, 2025, Prompt 1: \"Wat is de invoervereiste qua resolutie en tensorformaat voor ViT-architecturen, en hoe beïnvloeden resizing en normalisatie de performance van visuele encoders? Welke interpolatiemethoden behouden details bij downsampling naar 224x224 pixels?\"\n",
    "# Link: https://chatgpt.com/share/6857fcdf-2fbc-8001-ab19-efdbde61a083\n",
    "# Transformeer afbeeldingen naar inputgrootte voor model\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1dbbe9",
   "metadata": {},
   "source": [
    "## Laden van het voorgetrainde Vision-Text model\n",
    "\n",
    "In deze stap laden we een bestaand encoder-decoder model dat ontworpen is voor image captioning: het model vit-gpt2-image-captioning. Dit model combineert een Vision Transformer (ViT) als encoder en een GPT-2 taalmodel als decoder.\n",
    "\n",
    "De encoder zet de visuele informatie van een afbeelding om in een vectorrepresentatie. Deze representatie wordt vervolgens gebruikt door de decoder om een beschrijvende tekst te genereren. Omdat dit model reeds is voorgetraind op grote datasets met afbeeldings-bijschriftparen, kunnen we het effectief fine-tunen op onze kleinere dataset zonder dat we het model vanaf nul hoeven te trainen. Deze benadering valt onder transfer learning, waarbij eerder verworven kennis van een model wordt toegepast op een nieuwe taak, zoals beschreven door Howard en Ruder (2018).\n",
    "\n",
    "Daarnaast laden we ook de bijbehorende processor, die de afbeeldingen normaliseert en schaalt voordat ze naar het model gaan, en de tokenizer, die captions omzet naar tokens en omgekeerd.\n",
    "\n",
    "Bron:\n",
    "Howard, J., & Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (pp. 328–339). https://arxiv.org/abs/1801.06146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT, 2025, Prompt 1: \"Hoe werkt de encoder-decoderarchitectuur van VisionEncoderDecoderModel technisch? Welke interne mechanismen gebruiken ViT en GPT-2 voor cross-modal alignment? Wat zijn de implicaties van shared embeddings en layer freezing bij transfer learning in deze configuratie \"\n",
    "# Link: https://chatgpt.com/share/6857fd3c-7318-8001-8e5b-1636a7bd0163\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf9e3d",
   "metadata": {},
   "source": [
    "## Preprocessing per datapunt\n",
    "\n",
    "We definiëren een functie die preprocessing uitvoert op elk individueel datapunt uit de dataset. De afbeelding wordt getransformeerd naar een 224x224 tensor met behulp van de eerder gedefinieerde transformaties.\n",
    "\n",
    "Daarnaast wordt de bijbehorende caption omgezet naar een lijst van tokens met behulp van de tokenizer. De parameter max_length is ingesteld op 32 tokens. Deze waarde is gekozen op basis van verkennend onderzoek: de meeste captions in mode-datasets zijn tussen de 10 en 20 woorden lang. Door iets extra ruimte te laten, vermijden we dat langere captions worden afgekapt, terwijl we toch efficiënt omgaan met padding en rekentijd. Dit sluit aan bij de gemiddelde captionlengtes die zijn waargenomen in het COCO-datasetonderzoek (Lin et al., 2014).\n",
    "\n",
    "De gegenereerde labels bestaan uit integers die overeenkomen met de tokens in de caption. Deze worden tijdens het trainen vergeleken met de voorspellingen van het model.\n",
    "\n",
    "Bron:  \n",
    "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., ... & Dollár, P. (2014). Microsoft COCO: Common Objects in Context. *European Conference on Computer Vision (ECCV)*. https://arxiv.org/abs/1405.0312\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ea60f",
   "metadata": {},
   "source": [
    "## Preprocessing toepassen en dataset formatteren\n",
    "\n",
    "Met de .map()-functie passen we de preprocessing-functie toe op elk datapunt in zowel de trainings- als validatieset. Deze methode zorgt ervoor dat de afbeeldingen en captions automatisch worden omgezet naar het juiste formaat voor modelinput.\n",
    "\n",
    "Daarna gebruiken we .set_format() om expliciet aan te geven dat we PyTorch-tensors willen gebruiken en welke kolommen daarin nodig zijn. Door de kolommen te beperken tot 'pixel_values' en 'labels' vermijden we onnodige geheugenbelasting, wat cruciaal is bij training op beperkte hardware zoals Google Colab. Deze stap is verplicht om de gegevens correct te kunnen gebruiken in de PyTorch DataLoader in de volgende fase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8679f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT, 2025, Prompt 1: \"Hoe tokeniseer ik captiontekst op een manier die consistent is met de decoder van het model? Hoe verhoudt de keuze van max_length (32 tokens) zich tot de gemiddelde informatiedichtheid van productomschrijvingen in natuurlijke taal? Hoe vertaalt dit zich naar sequentielengte versus padding overhead?\"\n",
    "# ChatGPT, 2025, Prompt 2: \"Wat is het voordeel van .map() bij Hugging Face datasets boven for-loops? Hoe behoud ik efficiëntie bij gelijktijdige beeld- en tekstverwerking, en waarom moet ik de dataset herformateren naar een PyTorch-compatibele structuur met gespecificeerde kolommen (zoals pixel_values, labels)?\"\n",
    "# Link: https://chatgpt.com/share/6857fdd3-5518-8001-b6bd-ed3fd9c0cd75\n",
    "\n",
    "max_length = 32\n",
    "\n",
    "def preprocess(example):\n",
    "    image = image_transform(example[\"image\"])\n",
    "    caption = example[\"text\"]\n",
    "    labels = tokenizer(caption, padding=\"max_length\", truncation=True, max_length=max_length).input_ids\n",
    "    return {\"pixel_values\": image, \"labels\": labels}\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "val_dataset = val_dataset.map(preprocess)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2cd6c",
   "metadata": {},
   "source": [
    "## Train en validatie DataLoaders\n",
    "\n",
    "We gebruiken PyTorch's DataLoader om de voorbewerkte datasets in batches aan het model aan te bieden. Een batchgrootte van 8 is gekozen als een praktische balans tussen rekensnelheid en het beschikbare geheugen in Google Colab. Een te grote batchgrootte kan leiden tot geheugenfouten, terwijl een te kleine batch de training aanzienlijk vertraagt.\n",
    "\n",
    "De trainingsset wordt geshuffeld om te voorkomen dat het model leert op basis van de volgorde van de data. Door shuffling krijgt het model steeds andere combinaties van voorbeelden te zien, wat overfitting tegengaat en de generalisatie verbetert.\n",
    "\n",
    "Voor de validatieset wordt geen shuffling toegepast, zodat de evaluatie consistent en reproduceerbaar blijft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT, 2025, Prompt 1: \"Wat zijn de geheugeneffecten van het kiezen van een bepaalde batch size in een transformer-gebaseerde captioning pipeline?\"\n",
    "# Link: https://chatgpt.com/share/6857fe34-c27c-8001-8e16-29734fede4c4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c6c82",
   "metadata": {},
   "source": [
    "## Instellen van optimizer en device\n",
    "\n",
    "In deze stap kiezen we de optimizer en bepalen we op welk apparaat het model wordt getraind. We gebruiken de AdamW-optimizer, een variant van Adam met decoupled weight decay. Deze optimizer wordt vaak toegepast bij het trainen van transformer-gebaseerde modellen omdat het betere generalisatieprestaties oplevert dan standaard Adam. De weight decay voorkomt overfitting door gewichten af te straffen die te groot worden.\n",
    "\n",
    "De learning rate is ingesteld op 5e-5, wat een gangbare waarde is voor het fine-tunen van grote voorgetrainde modellen. Een lagere learning rate voorkomt dat het model abrupt zijn reeds aangeleerde kennis overschrijft, wat belangrijk is bij transfer learning.\n",
    "\n",
    "Het model wordt naar een GPU gestuurd als deze beschikbaar is. Het gebruik van een GPU zorgt voor veel snellere matrixvermenigvuldigingen en trainingsprocessen dan een CPU, vooral bij grote modellen en datasets.\n",
    "\n",
    "Bron:  \n",
    "Loshchilov, I., & Hutter, F. (2019). Decoupled Weight Decay Regularization. *International Conference on Learning Representations (ICLR)*. https://arxiv.org/abs/1711.05101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT, 2025, Prompt 1:Wat is het effect van weight decay in AdamW op het minimaliseren van loss zonder overfitting? Hoe verschilt decoupled weight decay mathematisch van traditionele L2-regularisatie? Waarom is een learning rate van 5e-5 stabiel bij fine-tuning van modellen met LayerNorm en Attention?\n",
    "# Link: https://chatgpt.com/share/6857fe7c-9edc-8001-92c7-66a581483367\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb744911",
   "metadata": {},
   "source": [
    "## Model trainen en evalueren met BLEU-score\n",
    "\n",
    "In deze stap trainen we het image captioning-model in meerdere rondes (epochs) en evalueren we het model na elke epoch met behulp van de BLEU-score.\n",
    "\n",
    "Tijdens training voorspelt het model een reeks woorden (tokens) op basis van de afbeelding. De fout tussen voorspelling en de echte caption wordt gemeten met de CrossEntropyLoss. Deze lossfunctie vergelijkt de gegenereerde tokens met de echte tokens op elke tijdstap.\n",
    "\n",
    "De formule voor Cross-Entropy Loss is:\n",
    "\n",
    "![CrossEntropyLoss](https://latex.codecogs.com/png.image?\\dpi{150}&space;L&space;=&space;-&space;\\sum_{i=1}^{N}&space;y_i&space;\\cdot&space;\\log(p_i))\n",
    "\n",
    "waarbij:\n",
    "- yᵢ = correcte waarde voor token i (one-hot encoded),\n",
    "- pᵢ = voorspelde kans voor token i.\n",
    "\n",
    "Een lagere loss betekent dat het model dichter bij de juiste output zit.\n",
    "\n",
    "### Evaluatie met BLEU\n",
    "\n",
    "Na elke epoch evalueren we de gegenereerde captions met de BLEU-score. BLEU (Bilingual Evaluation Understudy) vergelijkt n-grammen (reeksen van 1 t/m 4 woorden) van de gegenereerde caption met die van de referentiecaption.\n",
    "\n",
    "De BLEU-score wordt als volgt berekend:\n",
    "\n",
    "![BLEU-score](https://latex.codecogs.com/png.image?\\dpi{150}&space;BLEU&space;=&space;BP&space;\\cdot&space;\\exp\\left(\\sum_{n=1}^N&space;w_n&space;\\cdot&space;\\log(p_n)\\right))\n",
    "\n",
    "waarbij:\n",
    "- pₙ = de n-gram precisie voor n=1..4,\n",
    "- wₙ = het gewicht van elke n-gram (vaak 1/4),\n",
    "- BP = brevity penalty.\n",
    "\n",
    "De brevity penalty wordt toegepast om te korte zinnen te bestraffen:\n",
    "\n",
    "![BrevityPenalty](https://latex.codecogs.com/png.image?\\dpi{150}&space;BP&space;=&space;\\begin{cases}1&space;&,&space;c&space;>&space;r\\\\\\exp\\left(1&space;-&space;\\frac{r}{c}\\right)&space;&,&space;c&space;\\leq&space;r\\end{cases})\n",
    "\n",
    "waarbij:\n",
    "- c = lengte van de gegenereerde caption,\n",
    "- r = lengte van de referentiecaption.\n",
    "\n",
    "Een BLEU-score van 1.0 betekent een perfecte match. In de praktijk zijn scores tussen 0.2 en 0.5 gebruikelijk voor captioningmodellen. Door BLEU te gebruiken kunnen we objectief meten hoe goed het model inhoudelijk en taalkundig overeenkomt met de referentiecaption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT, 2025, Prompt 1: \"Hoe werkt de generate() methode bij VisionEncoderDecoderModel intern (greedy search, beam search)? Wat is de rol van cross entropy loss bij decoder training, en hoe worden logits gebruikt bij token sampling? Hoe wordt BLEU score gedefinieerd in termen van n-gram precisie, en welke gewichten gelden voor unigram t/m 4-gram evaluatie?\"\n",
    "# Link: https://chatgpt.com/share/6857fed7-60f8-8001-82e6-bb6fc6c10c8b\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training epoch {epoch+1}\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validatie met BLEU \n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for batch in tqdm(val_loader, desc=f\"Validatie epoch {epoch+1}\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(pixel_values, max_length=max_length)\n",
    "        decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "        preds.extend(decoded_preds)\n",
    "        refs.extend([[ref] for ref in decoded_labels])\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs)\n",
    "    print(f\"BLEU score (epoch {epoch+1}): {bleu_score['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3dafb5",
   "metadata": {},
   "source": [
    "## Caption genereren op basis van nieuwe afbeelding\n",
    "\n",
    "Deze functie laat het model een caption genereren op basis van een nieuwe afbeelding. De afbeelding wordt eerst op dezelfde manier getransformeerd als tijdens de training, waarna het model de tokens genereert met behulp van beam search of greedy decoding.\n",
    "\n",
    "Dit simuleert hoe het model in de praktijk gebruikt zou worden: een gebruiker voert een afbeelding in en ontvangt automatisch een tekstuele beschrijving. De caption wordt gegenereerd door de decoder op basis van de beeldrepresentatie van de encoder.\n",
    "\n",
    "Deze stap toont aan of het model daadwerkelijk in staat is om betekenisvolle zinnen te vormen die visueel relevante informatie bevatten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571132a7",
   "metadata": {},
   "source": [
    "## Vergelijking tussen gegenereerde en echte caption\n",
    "\n",
    "In deze stap selecteren we willekeurig een voorbeeld uit de validatieset en tonen we de afbeelding samen met de door het model gegenereerde caption en de originele referentie-caption.\n",
    "\n",
    "Door deze visuele vergelijking te maken krijgen we niet alleen inzicht in de BLEU-score of loss, maar zien we ook hoe goed het model inhoudelijk de afbeelding heeft geïnterpreteerd. Dit is belangrijk voor menselijke evaluatie: een caption kan grammaticaal correct zijn, maar inhoudelijk niet overeenkomen met wat er op de afbeelding staat.\n",
    "\n",
    "Deze vergelijking ondersteunt dus de kwalitatieve beoordeling van het model en is essentieel bij toepassingen zoals e-commerce, waar correcte beschrijvingen van kleding belangrijk zijn voor gebruikerservaring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT, 2025, Prompt 1: \"Hoe wordt een afbeelding op inference-structuurniveau verwerkt door ViT naar tekst door GPT-2 binnen een encoder-decoder-architectuur? \"\n",
    "# ChatGPT, 2025, Prompt 2: \"Hoe valideer ik de generalisatiecapaciteit van mijn model visueel en semantisch? Wat is het belang van directe vergelijking tussen modeloutput en referentiecaption, en hoe ondersteun ik die beoordeling kwalitatief naast metrische scores?\"\n",
    "# Link: https://chatgpt.com/share/6857ff63-c0b8-8001-8825-938ed388da3f\n",
    "\n",
    "def generate_caption(image_pil):\n",
    "    image_tensor = image_transform(image_pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(image_tensor, max_length=max_length)\n",
    "    caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Reset tijdelijk format om toegang te krijgen tot 'text'\n",
    "val_dataset.reset_format()\n",
    "\n",
    "# Random index\n",
    "random_index = random.randint(0, len(val_dataset) - 1)\n",
    "example = val_dataset[random_index]\n",
    "\n",
    "true_caption = example[\"text\"]\n",
    "\n",
    "# Zet list -> tensor -> PIL\n",
    "img_tensor = torch.tensor(example[\"pixel_values\"])\n",
    "img_pil = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "# Genereer caption\n",
    "generated_caption = generate_caption(img_pil)\n",
    "\n",
    "# Toon afbeelding + resultaten\n",
    "plt.imshow(img_pil)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Caption vergelijking\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model-caption    : {generated_caption}\")\n",
    "print(f\"Originele caption: {true_caption}\")\n",
    "\n",
    "# Zet format terug voor training\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783153fd",
   "metadata": {},
   "source": [
    "## Samenvatting van gekozen instellingen en onderbouwing\n",
    "\n",
    "| Hyperparameter / Instelling     | Gekozen waarde     | Onderbouwing                                                                                                   |\n",
    "|----------------------------------|---------------------|------------------------------------------------------------------------------------------------------------------|\n",
    "| **max_length**                   | 32                  | Captions in de dataset zijn gemiddeld 10–20 tokens lang. 32 biedt ruimte zonder overmatig padden. Gebaseerd op analyses van COCO-captionlengtes. |\n",
    "| **batch_size**                  | 8                   | Te grote batches veroorzaken geheugenproblemen in Colab. 8 biedt balans tussen performance en stabiliteit.     |\n",
    "| **learning_rate**               | 5e-5                | Kleine waarde voorkomt dat het voorgetrainde model zijn kennis verliest (catastrofaal vergeten). Aanbevolen voor transformer fine-tuning. |\n",
    "| **num_epochs**                  | 3                   | Voldoende om op kleine subset convergence te bereiken zonder overfitting. Training is stabiel en efficiënt.     |\n",
    "| **optimizer**                   | AdamW               | Werkt beter dan Adam in combinatie met weight decay (betere generalisatie). Gebaseerd op literatuur (Loshchilov & Hutter, 2019). |\n",
    "| **model**                       | ViT-GPT2 (nlpconnect/vit-gpt2-image-captioning) | Pre-trained encoder-decoder model specifiek ontwikkeld voor image captioning. Werkt direct met beelden en genereert natuurlijke taal. |\n",
    "| **dataset**                     | H&M Captions (Hugging Face) | Dataset bevat realistische productomschrijvingen voor mode. Perfect voor fine-tuning op beeld-bijschrift taken. |\n",
    "| **eval-metriek**                | BLEU (1–4 gram)     | Standaardmaat voor automatische tekstgeneratie. Meet overlap tussen gegenereerde tekst en referentiecaption.   |\n",
    "| **datasplitsing**              | 2000 training / 300 validatie | Om geheugenverbruik in Colab te beperken én het model toch effectief te trainen en valideren.                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8627b8",
   "metadata": {},
   "source": [
    "# Beeldgeneratie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6e5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srlam\\AppData\\Local\\Temp\\ipykernel_5824\\3926884792.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\srlam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/750 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Kies een voorgetraind BLIP model voor image captioning\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Voorbeeld preprocessing functie\n",
    "def preprocess(example):\n",
    "    inputs = processor(images=example[\"image\"], text=example[\"text\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    inputs = {k: v.squeeze() for k, v in inputs.items()}\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Gebruik with_transform voor memory-efficiënte preprocessing\n",
    "train_tokenized_dataset = train_dataset.with_transform(preprocess)\n",
    "val_tokenized_dataset = val_dataset.with_transform(preprocess)\n",
    "\n",
    "# Stel training parameters in\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-finetuned-hm\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False \n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=val_tokenized_dataset,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "\n",
    "# Fine-tune het model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b3888",
   "metadata": {},
   "source": [
    "## Modelarchitectuur en Pretraining\n",
    "\n",
    "Voor deze taak is gekozen voor de **BLIP (Bootstrapped Language-Image Pretraining)** modelarchitectuur, specifiek het model `Salesforce/blip-image-captioning-base`. Dit model is vooraf getraind op grote multimodale datasets zoals **Conceptual Captions** en **LAION-400M**, waarbij het leert om afbeeldingen en tekstuele beschrijvingen aan elkaar te koppelen.\n",
    "\n",
    "## Taak: Tekst-naar-beeld generatie\n",
    "\n",
    "Het model wordt aangepast zodat het een **Engels tekstbijschrift als input** neemt en een **afbeelding als output** genereert. In deze notebook wordt BLIP gebruikt voor image captioning, maar voor tekst-naar-beeld generatie zou een model als **Stable Diffusion** of **DALL·E** meer geschikt zijn. BLIP kan echter ook multimodale taken uitvoeren, zoals het genereren van afbeeldingen op basis van tekst (mits aangepast).\n",
    "\n",
    "## Stappen bij het finetunen\n",
    "\n",
    "1. **Dataset laden:** De H&M fashion captioning dataset wordt geladen en gesplitst in train/validatie.\n",
    "2. **Preprocessing:** Elke sample wordt verwerkt met de BLIP processor, waarbij de tekst en afbeelding worden omgezet naar tensors die het model kan gebruiken.\n",
    "3. **Tokenisatie:** De tekst wordt getokeniseerd en samen met de afbeelding als input aan het model gegeven.\n",
    "4. **Training setup:** Trainingsparameters worden ingesteld, zoals batch size, aantal epochs, learning rate, enz.\n",
    "5. **Trainer:** De Huggingface `Trainer` wordt gebruikt om het model te finetunen op de trainingsdata.\n",
    "6. **Validatie:** Tijdens training wordt het model geëvalueerd op de validatieset.\n",
    "\n",
    "## Componenten van het model\n",
    "\n",
    "- **Vision Encoder:** Een visueel neuraal netwerk (ViT) dat afbeeldingsfeatures extraheert.\n",
    "- **Text Encoder:** Een transformer die tekstuele input verwerkt.\n",
    "- **Cross-modal Transformer:** Combineert visuele en tekstuele informatie.\n",
    "- **Decoder:** Genereert de output (bijvoorbeeld een caption of, bij uitbreiding, een afbeelding).\n",
    "\n",
    "## Hoe maakt het model afbeeldingen en welke lossfunctie gebruikt het?\n",
    "\n",
    "Het BLIP-model leert een koppeling tussen tekst en beeld. Bij image captioning genereert het tekst bij een afbeelding; bij tekst-naar-beeld (mits aangepast) zou het een afbeelding genereren die past bij de tekst. De **lossfunctie** is meestal een combinatie van cross-entropy loss voor tekstgeneratie en contrastieve loss voor multimodale matching.\n",
    "\n",
    "## Keuze van hyperparameters, optimizer en epochs\n",
    "\n",
    "- **Batch size:** 8, gekozen voor een balans tussen snelheid en geheugengebruik.\n",
    "- **Epochs:** 3, voldoende om te finetunen zonder overfitting op een relatief kleine dataset.\n",
    "- **Learning rate:** 5e-5, standaardwaarde voor finetuning van transformer-gebaseerde modellen.\n",
    "- **Optimizer:** AdamW, omdat deze goed werkt voor grote taal- en visiemodellen door adaptieve learning rates en gewichtsregularisatie.\n",
    "\n",
    "Deze keuzes zijn gebaseerd op best practices voor het finetunen van grote multimodale modellen en de grootte van de dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fde5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def fashion_app(image_path, detection_model, caption_model, tokenizer, image_transform, class_names, device, max_length=32):\n",
    "    # 1. Detectie: Voorspel bounding boxes en klassen\n",
    "    results = detection_model(image_path)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]\n",
    "    class_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "    scores = results[0].boxes.conf.cpu().numpy()\n",
    "    \n",
    "    # 2. Laad afbeelding\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # 3. Voor elk gedetecteerd object: crop, caption, plot\n",
    "    captions = []\n",
    "    for i, (box, class_id, score) in enumerate(zip(boxes, class_ids, scores)):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cropped = image.crop((x1, y1, x2, y2))\n",
    "        # Caption genereren\n",
    "        image_tensor = image_transform(cropped).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = caption_model.generate(image_tensor, max_length=max_length)\n",
    "        caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        captions.append((class_names[class_id], caption))\n",
    "        # Plot bounding box en caption\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-10, f\"{class_names[class_id]}: {caption}\", color='white', fontsize=10,\n",
    "                bbox=dict(facecolor='black', alpha=0.7, edgecolor='none'))\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    return captions\n",
    "\n",
    "# --- Gebruik de app op een voorbeeldafbeelding ---\n",
    "# Pas het pad hieronder aan naar jouw testafbeelding\n",
    "test_image_path = \"voorbeeld.jpg\"  # <-- Vervang door jouw afbeelding\n",
    "\n",
    "# Voer de app uit\n",
    "resultaten = fashion_app(\n",
    "    image_path=test_image_path,\n",
    "    detection_model=model,           # YOLOv8 model\n",
    "    caption_model=model,             # Captioning model (ViT-GPT2)\n",
    "    tokenizer=tokenizer,\n",
    "    image_transform=image_transform,\n",
    "    class_names=CLASS_NAMES,\n",
    "    device=device,\n",
    "    max_length=32\n",
    ")\n",
    "\n",
    "# Print resultaten\n",
    "for cls, cap in resultaten:\n",
    "    print(f\"{cls}: {cap}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
